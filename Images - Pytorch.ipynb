{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Basic \n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Scikit-learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow as tf, re, math\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "import efficientnet.tfkeras as efn\n",
    "from keras import layers\n",
    "from keras.applications import DenseNet121\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Visualization\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from plotly.offline import iplot\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Misc\n",
    "\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import cufflinks\n",
    "import cv2 as cv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import albumentations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from wtfml.utils import EarlyStopping\n",
    "from wtfml.engine import Engine\n",
    "from wtfml.data_loaders.image import ClassificationDataLoader\n",
    "\n",
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEResnext50_32x4d(nn.Module):\n",
    "    def __init__(self, pretrained='imagenet'):\n",
    "        super(SEResnext50_32x4d, self).__init__()\n",
    "        \n",
    "        self.base_model = pretrainedmodels.__dict__[\n",
    "            \"se_resnext50_32x4d\"\n",
    "        ](pretrained=None)\n",
    "        if pretrained is not None:\n",
    "            self.base_model.load_state_dict(\n",
    "                torch.load(\n",
    "                    \"se_resnext50_32x4d-a260b3a4.pth\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.l0 = nn.Linear(2048, 1)\n",
    "    \n",
    "    def forward(self, image, targets):\n",
    "        batch_size, _, _, _ = image.shape\n",
    "        \n",
    "        x = self.base_model.features(image)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n",
    "        \n",
    "        out = self.l0(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(out, targets.view(-1, 1).type_as(x))\n",
    "\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting file paths for our notebook:\n",
    "\n",
    "base_path = r'C:\\Users\\HIMANSHU\\Downloads\\Melanoma jpg'\n",
    "\n",
    "train_img_path = r'C:\\Users\\HIMANSHU\\Downloads\\Melanoma jpg\\train'\n",
    "train_images = [f for f in listdir(train_img_path) if isfile(join(train_img_path, f))]\n",
    "\n",
    "test_img_path = r'C:\\Users\\HIMANSHU\\Downloads\\Melanoma jpg\\test'\n",
    "test_images = [f for f in listdir(test_img_path) if isfile(join(test_img_path, f))]\n",
    "\n",
    "img_stats_path = r'C:\\Users\\HIMANSHU\\Downloads\\Melanoma Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folds\n",
    "df = pd.read_csv(os.path.join(base_path, 'train.csv'))\n",
    "df[\"kfold\"] = -1    \n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.target.values\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f\n",
    "\n",
    "df.to_csv(\"train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold):\n",
    "    training_data_path = r'C:\\Users\\HIMANSHU\\Downloads\\Melanoma jpg\\train'\n",
    "    df = pd.read_csv(\"train_folds.csv\")\n",
    "    device = \"cuda\"\n",
    "    epochs = 50\n",
    "    train_bs = 32\n",
    "    valid_bs = 16\n",
    "\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    model = SEResnext50_32x4d(pretrained=\"imagenet\")\n",
    "    model.to(device)\n",
    "\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    train_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
    "            albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15),\n",
    "            albumentations.Flip(p=0.5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    valid_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_images = df_train.image_name.values.tolist()\n",
    "    train_images = [os.path.join(training_data_path, i + \".png\") for i in train_images]\n",
    "    train_targets = df_train.target.values\n",
    "\n",
    "    valid_images = df_valid.image_name.values.tolist()\n",
    "    valid_images = [os.path.join(training_data_path, i + \".png\") for i in valid_images]\n",
    "    valid_targets = df_valid.target.values\n",
    "\n",
    "    train_dataset = ClassificationDataLoader(\n",
    "        image_paths=train_images,\n",
    "        targets=train_targets,\n",
    "        resize=None,\n",
    "        augmentations=train_aug,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=train_bs, shuffle=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = ClassificationDataLoader(\n",
    "        image_paths=valid_images,\n",
    "        targets=valid_targets,\n",
    "        resize=None,\n",
    "        augmentations=valid_aug,\n",
    "    )\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=valid_bs, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        patience=3,\n",
    "        threshold=0.001,\n",
    "        mode=\"max\"\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=5, mode=\"max\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = Engine.train(train_loader, model, optimizer, device=device)\n",
    "        predictions, valid_loss = Engine.evaluate(\n",
    "            valid_loader, model, device=device\n",
    "        )\n",
    "        predictions = np.vstack((predictions)).ravel()\n",
    "        auc = metrics.roc_auc_score(valid_targets, predictions)\n",
    "        print(f\"Epoch = {epoch}, AUC = {auc}\")\n",
    "        scheduler.step(auc)\n",
    "\n",
    "        es(auc, model, model_path=f\"model_fold_{fold}.bin\")\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(fold):\n",
    "    test_data_path = r'C:\\Users\\HIMANSHU\\Downloads\\Melanoma jpg\\test'\n",
    "    df = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
    "    device = \"cuda\"\n",
    "    model_path=f\"model_fold_{fold}.bin\"\n",
    "\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    images = df.image_name.values.tolist()\n",
    "    images = [os.path.join(test_data_path, i + \".png\") for i in images]\n",
    "    targets = np.zeros(len(images))\n",
    "\n",
    "    test_dataset = ClassificationDataLoader(\n",
    "        image_paths=images,\n",
    "        targets=targets,\n",
    "        resize=None,\n",
    "        augmentations=aug,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=16, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    model = SEResnext50_32x4d(pretrained=None)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = Engine.predict(test_loader, model, device=device)\n",
    "    predictions = np.vstack((predictions)).ravel()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-05 11:33:21.592 INFO    numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'ClassificationDataLoader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-898965d5fbd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d996442dc980>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(fold)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     train_loader = torch.utils.data.DataLoader(\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_bs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[0;32m    260\u001b[0m                     \u001b[1;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    100\u001b[0m                              \"since a random permute will be performed.\")\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m    104\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36mnum_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# dataset size might change at runtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'ClassificationDataLoader' has no len()"
     ]
    }
   ],
   "source": [
    "train(0)\n",
    "train(1)\n",
    "train(2)\n",
    "train(3)\n",
    "train(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
